---
- name: Clean up filesystem signature
  command: wipefs -a {{ item }}
  with_items: "{{ disks | default([]) }}"
  when:  wipefs == 'yes' and item is defined
  ignore_errors: yes

# Set data alignment for JBODs, by default it is 256K. This set_fact is not
# needed if we can always assume 256K for JBOD, however we provide this extra
# variable to override it.
- name: Set PV data alignment for JBOD
  set_fact:
    pv_dataalign: "{{ gluster_infra_dalign | default('256K') }}"
  when: disktype == 'NONE' or disktype == 'RAID0'

# Set data alignment for RAID
# We need KiB: ensure to keep the trailing `K' in the pv_dataalign calculation.
- name: Set PV data alignment for RAID
  set_fact:
    pv_dataalign: >
        {{ diskcount|int *
           stripesize|int }}K
  when: >
      disktype == 'RAID6' or
      disktype == 'RAID10'

- name: Set VG physical extent size for RAID
  set_fact:
    vg_pesize: >
         {{ diskcount|int *
            stripesize|int }}K
  when: >
     disktype == 'RAID6' or
     disktype == 'RAID10'

- name: Create volume groups
  lvg:
    state: present
    vg: "{{ vgname }}"
    pvs: "{{ disks }}"
    pv_options: "--dataalignment {{ pv_dataalign }}"
    # pesize is 4m by default for JBODs
    pesize: "{{ vg_pesize | default(4) }}"

# Chunksize is calculated as follows for GlusterFS' optimal performance.
# RAID6:
#    full_stripe_size = stripe_unit_size * no_of_data_disks
#
# Where full_stripe_size should be between 1 MiB and 2 MiB. And chunksize is set
# to full_stripe_size
#
- name: Calculate chunksize for RAID6/RAID10
  set_fact:
     lv_chunksize: >
        {{ stripesize|int *
           diskcount|int }}K
  when: >
     disktype == 'RAID6' or
     disktype == 'RAID10'

# For JBOD the thin pool chunk size is set to 256 KiB.
- name: Set chunksize for JBOD
  set_fact:
     lv_chunksize: '256K'
  when: disktype == 'NONE' or disktype == 'RAID0'

- name: Create a LV thinpool
  lvol:
     state: present
     shrink: false
     vg: "{{ vgname }}"
     thinpool: "{{ lvname }}_pool"
     size: 100%FREE
     opts: " --chunksize {{ lv_chunksize }}
             --poolmetadatasize {{ pool_metadatasize }}
             --zero n"

- name: Create thin logical volume
  lvol:
     state: present
     vg: "{{ vgname }}"
     shrink: no
     lv: "{{ lvname }}"
     size: "{{ size }}"
     thinpool: "{{ lvname }}_pool"

- include_tasks: lvmcache.yml
  when: ssd is defined and ssd

- name: Create an xfs filesystem
  filesystem:
    fstype: "{{ fstype }}"
    dev: "/dev/{{ vgname }}/{{ lvname }}"
    opts: "{{ fsopts }}{{ raidopts }}"
  vars:
    fsopts: "-f -K -i size=512 -n size=8192"
    raidopts: "{% if 'raid' in disktype %} -d sw={{ diskcount }},su={{ stripesize }}k {% endif %}"

- name: Create the backend directory, skips if present
  file:
    path: "{{ mntpath }}"
    state: directory

- name: Mount the brick
  mount:
    name: "{{ mntpath }}"
    src: "/dev/{{ vgname }}/{{ lvname }}"
    fstype: "{{ fstype }}"
    opts: "inode64,noatime,nodiratime"
    state: mounted

- name: Set SELinux labels on the bricks
  sefcontext:
    target: "{{ mntpath }}"
    setype: glusterd_brick_t
    state: present
    reload: yes
